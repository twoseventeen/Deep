{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/jihyeon/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/libmxnet.so, 0x0006): tried: '/Users/jihyeon/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/libmxnet.so' (not a mach-o file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgluonnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencepieceTokenizer\n",
      "File \u001b[0;32m~/miniforge3/envs/final/lib/python3.8/site-packages/gluonnlp/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"NLP toolkit.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"MXNet: a concise, fast and flexible framework for deep learning.\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Context, current_context, cpu, gpu, cpu_pinned\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m engine\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MXNetError\n",
      "File \u001b[0;32m~/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/context.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classproperty, with_metaclass, _MXClassPropertyMetaClass\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_call\n",
      "File \u001b[0;32m~/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/base.py:214\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# library instance of mxnet\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# type definitions\u001b[39;00m\n\u001b[1;32m    217\u001b[0m mx_int \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int\n",
      "File \u001b[0;32m~/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/base.py:205\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"Load library by searching possible path.\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39mfind_lib_path()\n\u001b[0;32m--> 205\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_LOCAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# DMatrix functions\u001b[39;00m\n\u001b[1;32m    207\u001b[0m lib\u001b[38;5;241m.\u001b[39mMXGetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n",
      "File \u001b[0;32m~/miniforge3/envs/final/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/jihyeon/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/libmxnet.so, 0x0006): tried: '/Users/jihyeon/miniforge3/envs/final/lib/python3.8/site-packages/mxnet/libmxnet.so' (not a mach-o file)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer\n",
    "from gluonnlp.data import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/KOR'\n",
    "TRAIN_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-train.tsv')\n",
    "train_data = pd.read_csv(TRAIN_STS_DF, sep='\\t', quoting = 3)\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('전체 train_data 개수: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.Series(train_data['sentence1'].tolist() + train_data['sentence2'].tolist()).astype(str)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('전체 문장 데이터의 개수: {}'.format(len(train_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('유일한 총 문장 수 : {}'.format(len(np.unique(train_set))))\n",
    "print('반복해서 나타나는 문장의 수: {}'.format(np.sum(train_set.value_counts() > 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(12, 5))\n",
    "# 히스토그램 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위\n",
    "# range: x축 값의 범위\n",
    "# alpha: 그래프 색상 투명도\n",
    "# color: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_set.value_counts(), bins=50, alpha=0.5, color= 'r', label='word')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "# 그래프 제목\n",
    "plt.title('Log-Histogram of sentence appearance counts')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of occurrences of sentence')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Number of sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('중복 최대 개수: {}'.format(np.max(train_set.value_counts())))\n",
    "print('중복 최소 개수: {}'.format(np.min(train_set.value_counts())))\n",
    "print('중복 평균 개수: {:.2f}'.format(np.mean(train_set.value_counts())))\n",
    "print('중복 표준편차: {:.2f}'.format(np.std(train_set.value_counts())))\n",
    "print('중복 중간길이: {}'.format(np.median(train_set.value_counts())))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 중복: {}'.format(np.percentile(train_set.value_counts(), 25)))\n",
    "print('제 3 사분위 중복: {}'.format(np.percentile(train_set.value_counts(), 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot([train_set.value_counts()],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = train_set.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_length, bins=200, range=[0,200], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Normalised histogram of character count in sentence\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of characters', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문장 길이 최대 값: {}'.format(np.max(train_length)))\n",
    "print('문장 길이 평균 값: {:.2f}'.format(np.mean(train_length)))\n",
    "print('문장 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
    "print('문장 길이 중간 값: {}'.format(np.median(train_length)))\n",
    "print('문장 길이 제 1 사분위: {}'.format(np.percentile(train_length, 25)))\n",
    "print('문장 길이 제 3 사분위: {}'.format(np.percentile(train_length, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(train_length,\n",
    "             labels=['char counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_counts = train_set.apply(lambda x:len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_word_counts, bins=50, range=[0, 50], facecolor='r', density=True, label='train')\n",
    "plt.title('Normalised histogram of word count in sentence', fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Prabability', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문장 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))\n",
    "print('문장 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))\n",
    "print('문장 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
    "print('문장 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))\n",
    "print('문장 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
    "print('문장 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))\n",
    "print('문장 단어 개수 99 퍼센트: {}'.format(np.percentile(train_word_counts, 99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(train_word_counts,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmarks = np.mean(train_set.apply(lambda x: '?' in x)) # 물음표가 구두점으로 쓰임\n",
    "math = np.mean(train_set.apply(lambda x: '[math]' in x)) # []\n",
    "fullstop = np.mean(train_set.apply(lambda x: '.' in x)) # 마침표\n",
    "capital_first = np.mean(train_set.apply(lambda x: x[0].isupper())) #  첫번째 대문자\n",
    "capitals = np.mean(train_set.apply(lambda x: max([y.isupper() for y in x]))) # 대문자가 몇개\n",
    "numbers = np.mean(train_set.apply(lambda x: max([y.isdigit() for y in x]))) # 숫자가 몇개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('물음표가있는 문장: {:.2f}%'.format(qmarks * 100))\n",
    "print('마침표를 포함한 문장: {:.2f}%'.format(fullstop * 100))\n",
    "print('첫 글자가 대문자 인 문장: {:.2f}%'.format(capital_first * 100))\n",
    "print('대문자가있는 문장: {:.2f}%'.format(capitals * 100))\n",
    "print('숫자가있는 문장: {:.2f}%'.format(numbers * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_token_counts = train_set.apply(lambda x:len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_bert_token_counts, bins=200, range=[0, 200], facecolor='r', density=True, label='train')\n",
    "plt.title('Normalised histogram of tokens count in sentence', fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of tokens', fontsize=15)\n",
    "plt.ylabel('Prabability', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문장 tokens 개수 최대 값: {}'.format(np.max(train_bert_token_counts)))\n",
    "print('문장 tokens 개수 평균 값: {:.2f}'.format(np.mean(train_bert_token_counts)))\n",
    "print('문장 tokens 개수 표준편차: {:.2f}'.format(np.std(train_bert_token_counts)))\n",
    "print('문장 tokens 개수 중간 값: {}'.format(np.median(train_bert_token_counts)))\n",
    "print('문장 tokens 개수 제 1 사분위: {}'.format(np.percentile(train_bert_token_counts, 25)))\n",
    "print('문장 tokens 개수 제 3 사분위: {}'.format(np.percentile(train_bert_token_counts, 75)))\n",
    "print('문장 tokens 개수 99 퍼센트: {}'.format(np.percentile(train_bert_token_counts, 99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(train_bert_token_counts,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_token_cased_counts = train_set.apply(lambda x:len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_bert_token_cased_counts, bins=100, range=[0, 100], facecolor='r', density=True, label='train')\n",
    "plt.title('Normalised histogram of tokens count in sentence', fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of tokens', fontsize=15)\n",
    "plt.ylabel('Prabability', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문장 tokens 개수 최대 값: {}'.format(np.max(train_bert_token_cased_counts)))\n",
    "print('문장 tokens 개수 평균 값: {:.2f}'.format(np.mean(train_bert_token_cased_counts)))\n",
    "print('문장 tokens 개수 표준편차: {:.2f}'.format(np.std(train_bert_token_cased_counts)))\n",
    "print('문장 tokens 개수 중간 값: {}'.format(np.median(train_bert_token_cased_counts)))\n",
    "print('문장 tokens 개수 제 1 사분위: {}'.format(np.percentile(train_bert_token_cased_counts, 25)))\n",
    "print('문장 tokens 개수 제 3 사분위: {}'.format(np.percentile(train_bert_token_cased_counts, 75)))\n",
    "print('문장 tokens 개수 99 퍼센트: {}'.format(np.percentile(train_bert_token_cased_counts, 99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(train_bert_token_cased_counts,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentencepeice_counts = train_set.apply(lambda x:len(tokenizer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_sentencepeice_counts, bins=100, range=[0, 100], facecolor='r', density=True, label='train')\n",
    "plt.title('Normalised histogram of tokens count in sentence', fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of tokens', fontsize=15)\n",
    "plt.ylabel('Prabability', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문장 tokens 개수 최대 값: {}'.format(np.max(train_sentencepeice_counts)))\n",
    "print('문장 tokens 개수 평균 값: {:.2f}'.format(np.mean(train_sentencepeice_counts)))\n",
    "print('문장 tokens 개수 표준편차: {:.2f}'.format(np.std(train_sentencepeice_counts)))\n",
    "print('문장 tokens 개수 중간 값: {}'.format(np.median(train_sentencepeice_counts)))\n",
    "print('문장 tokens 개수 제 1 사분위: {}'.format(np.percentile(train_sentencepeice_counts, 25)))\n",
    "print('문장 tokens 개수 제 3 사분위: {}'.format(np.percentile(train_sentencepeice_counts, 75)))\n",
    "print('문장 tokens 개수 99 퍼센트: {}'.format(np.percentile(train_sentencepeice_counts, 99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(train_sentencepeice_counts,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "font_path = os.path.join(DATA_IN_PATH, 'NanumGothic.ttf')\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(\" \".join(train_set.astype(str)))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.genre.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.filename.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.year.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
