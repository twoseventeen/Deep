{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIT\\miniconda3\\envs\\3.6ver2\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\BIT\\miniconda3\\envs\\3.6ver2\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\BIT\\miniconda3\\envs\\3.6ver2\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "from random import sample, seed\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "seed(1234)\n",
    "phoneme_tokenizer = Okt()\n",
    "\n",
    "DATA_IN_PATH = './data_in/KOR'\n",
    "save_path = \"bert_ckpt/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lowercase=False).save_pretrained(save_path)\n",
    "\n",
    "bert_tokenizer = BertWordPieceTokenizer(\"bert_ckpt/vocab.txt\", lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\"\n",
    "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./precedent.json\"\n",
    "\n",
    "with open(file_path, 'rt', encoding=\"UTF-8\") as file:\n",
    "    data = json.load(file)\n",
    "    print(type(data))\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xed in position 65: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mE:\\T\\bit\\ipykernel_220\\903498101.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\3.6ver2\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xed in position 65: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "train_data = json.load(open(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(train_data['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지문 텍스트 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for d in train_data['data']:\n",
    "    documents += [p['context'] for p in d['paragraphs']]\n",
    "print('전체 텍스트 수: {}'.format(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지문 텍스트 어절 단위 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_docs = []\n",
    "for d in documents:\n",
    "    len_docs.append(len(d.split()))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_docs)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_docs)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_docs)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_docs)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_docs)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_docs, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_docs, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot([len_docs],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_len_docs = [l for l in len_docs if l < 2000]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot(filtered_len_docs,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(filtered_len_docs, bins=150, range=[0,600], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of word count in paragraph\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지문 텍스트 버트 토크나이저 토큰 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_bert_tokenized_docs = []\n",
    "for d in documents:\n",
    "    len_bert_tokenized_docs.append(len(bert_tokenizer.encode(d, add_special_tokens=False).tokens))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_bert_tokenized_docs)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_bert_tokenized_docs)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_bert_tokenized_docs)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_bert_tokenized_docs)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_bert_tokenized_docs)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_docs, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_docs, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_len_bert_tokenized_docs = [l for l in len_bert_tokenized_docs if l < 3000]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(filtered_len_bert_tokenized_docs,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(filtered_len_bert_tokenized_docs, bins=150, range=[0,1000], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of Bert Tokenizer token count in paragraph\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지문 텍스트 어휘 빈도 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어절 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for d in documents:\n",
    "    sentences += sent_tokenize(d)\n",
    "\n",
    "print('전체 문장 수: {}'.format(len(sentences)))\n",
    "sampled_docs = sample(sentences, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = os.path.join(DATA_IN_PATH, 'NanumGothic.ttf')\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(sampled_docs))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extracted_docs = []\n",
    "for d in sampled_docs:\n",
    "    noun_extracted_docs += phoneme_tokenizer.nouns(d)\n",
    "\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(noun_extracted_docs))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for d in train_data['data']:\n",
    "    qas = [p['qas'] for p in d['paragraphs']]\n",
    "    for c in qas:\n",
    "        questions += [q['question'] for q in c]\n",
    "    \n",
    "print('전체 질문 수: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 텍스트 어절 토큰 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_qs = []\n",
    "for q in questions:\n",
    "    len_qs.append(len(q.split()))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_qs)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_qs)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_qs)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_qs)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_qs)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_qs, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_qs, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot([len_qs],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(len_qs, bins=50, range=[0,50], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of word count in sentence\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 텍스트 버트 토크나이저 토큰 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_bert_tokenized_q = []\n",
    "for q in questions:\n",
    "    len_bert_tokenized_q.append(len(bert_tokenizer.encode(q, add_special_tokens=False).tokens))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_bert_tokenized_q)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_bert_tokenized_q)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_bert_tokenized_q)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_bert_tokenized_q)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_bert_tokenized_q)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_q, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_q, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot([len_bert_tokenized_q],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(len_bert_tokenized_q, bins=50, range=[0,50], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of Bert Tokenizer token count in sentence\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 텍스트 어휘 빈도 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어절 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_questions = sample(questions, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(sampled_questions))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extracted_qs = []\n",
    "for q in sampled_questions:\n",
    "    noun_extracted_qs += phoneme_tokenizer.nouns(q)\n",
    "\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(noun_extracted_qs))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
